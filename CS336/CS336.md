# Assignment 1 - Basics:
* Implement BPE tokenizer
* Implement Transformer, cross-entropy loss, AdamW optimizer, training loop
* Train on TinyStories and OpenWebText
* Leaderboard: minimize OpenWebText perplexity given 90 mins on a H100


# Sytems:
* optimizing for efifciency
* learn kernels, parallelism, inference GPU (A100)
* FLOPS, GPU chip, memory, L1, L2 cache
* how to mimize data movement from DRAM <--> SRAM + Compute
  * write kernels in CUDA/Triton
* multiple GPUs, (8 A100s)
  * data movement even slower
  * use colelctive operations (gather, reduce, all-reduce)
  * shard across GPUs
  * split computation parallelism

# Inference:
* how to actuaklly use the model
* also needed for RL, test-time compute, evaluation
* inference computer exceeds training compute
* KV-Cache
* prefill (prompt and forward) and decode (token to word one by one)
* methods to speed up decoding:
  * cheaper model to speed up (model pruning, quantization, distillation)
  * KV caching, batching

# Assignment 2 - Systems
* Implement a fused RMSNorm Kernel
* Implement distributed data parallel training
* Implement optimizer state sharding
* Benchmark and profile the implementations **most important. need to benchmark compare**


# Scaling Laws:
* do experimenats at small scale, predict hyperparams/loss at large scale
* given a FLOPs budget, use bigger model & less data? or more data & smaller model?
* whats the right balance?
  Compute-Optimal Scaling Laws:
    * Kaplan+ 2020 Scaling laws paper
    * rule of thumb: 20 x Number of Params = Tokesn to train on
    (e.g. 1.4B param model should be trained on 28B tokens)


# Assingment 3 - FLOPs budget scaling law
* fit scaling law to the data points
* minimize loss given FLOPs budget


# Evaluation:
* Perplexity: textbook evaluation for language models
* Standardized testing (MMLU, HellaSwag, GSM8k)
* Instriction following (AlpacaEval, IFEval, WildBench)
* Scaling test-time compute: chain of thougth, ensembling
* LM as a judge
* Full systems: RAG, agents

--- By now should have a Base Model ---
* raw potential
* completion model
* need alignment



# Alignment:
* follow instructions
* tune the style (format, length, tone, etc)
* incorporate safety

1. supervised finetuning:
    template. user -> role -> content
    ~ 1000 examples is enough
    human manual annotation
    fine-tune model to maximize p(response | prompt)

2. learning from feedback:
    improve without annotation
    * preference data (A or B response is better)
    * verifiers (LM as a judge)
    * RL: PPO, DPO, GRPO

# Assignment 5 - Post Training:
  * implement supervised fine-tuning
  * implement DPO
  * implement GRPO


# Efficiency:
* Data procesisng: avoid wasting compute on bad data
* Tokenization: raw bytes is inefficient. so tokenize
* Model architecture: reduce memory or FLOPs (KV caches, sliding window attention)
* Training: single epoch can do sometimes
* Scaling laws: less compute on smaller models to do hyperparams tuning
* Alignment: if specialized task, then use smaller model (create a path to go to A. very specialized task)



# Simple BPE Tokenizer
string to bytes
merge bytes with BP algorithm

BP algo:
    * loop indices of  bytes 1 - 256 bytes
    * count adjacent pairs (a, b): count
    * find pair with most occurence
    * merge that pair
    * give new indice to that pair i,e (a,b) pair becomes 256
    * the indices list should will shrink as pairs become merged

# Resource, Memory (GB) & Compute (FLOPs)
* all the params are stored as floating point numbers
* Memory determined by:
  1. number of values
  2. data type

* store params with fp32 storing optimizers, params state
* train with bf16 for computations feedforward


# Parameter Initialization
* normalize 1/np.sqrt(num_inputs)
* large valies can cause gradients blowup and unstable training
old:
* w = nn.Parameter(torch.randn(input_dim))
new:
* w = nn.Parameter(torch.randn(input, hidden_dim) / np.sqrt(input_dim))

further truncate [3, 3]
w = nn.Parameter(nn.init.runc_nromal_(torch.empty(input_dim, hidden_dim), std=1 / np.sqrt(input_dim)))

# How much memory required for training
total_memory = 4 * (num_params + num_activations + num_grads + num_opt)
flops = 6 * B * num_params (6: 2 forward, 4 backwards)

# Checkpointing
* save the checkpoint to hedge crashes
* save the model & optimizer

* bf16/fp8 for forward, f32 for backwards, gradients.


# Shift in Architectures in practice
* post-norm -> pre-norm, more stable
* layernorm -> RMSNorm: removes the mean and bias. saves data movement/memory, fewer operations & params
* dropping bias terms. FFN(x) = max(0, xW1 + b1)W2 + b2  --->  FFN(x) = sigma(xW1)W2: memory, stability

* relu -> Swiglu
* sine -> ROPE
* ROPE for position embedding instead of sine/cosine. applies Theta rotate to vector by its position.

# monolingual models - vocab specialized
~ 30-50k tokens size
* original transformer: 37000
* gpt: 40257


# Operation / Memory Efficiency
* KV-Cache -> MQA -> GQA
* KV Cache has too many data movement
